# ===================================================================
# KI-WISSENSSYSTEM - LiteLLM "Smart Alias" Configuration
# Enterprise-Ready Single-Alias Strategy (Version 7.0)
# Direct Task-to-Model Mapping for Maximum UI Usability
# ===================================================================

# Global Configuration - Production Optimized
router_settings:
  enable_pre_call_checks: true
  enable_file_based_routing: true
  enable_loadbalancing: true
  cooldown_time: 30
  timeout: 30.0
  max_retries: 3
  routing_strategy: usage-based-routing-v2
  redis_host: "redis"
  redis_port: 6379
  redis_password: null
  
# Performance & Security Features (v1.72.6 optimizations)
general_settings:
  master_key: "os.environ/LITELLM_MASTER_KEY" # âœ… PRODUCTION SECURITY
  database_url: "os.environ/DATABASE_URL"
  store_model_in_db: true
  
  # Production Best Practices
  alerting: ["slack"]
  proxy_batch_write_at: 60
  database_connection_pool_limit: 10
  disable_error_logs: true
  allow_requests_on_db_unavailable: true
  
  # Performance Optimizations (v1.72.0+ features)
  use_aiohttp_transport: true
  disable_spend_logs: false
  disable_router_logs: false
  
  # Rate Limiting & Multi-instance Improvements
  experimental_enable_multi_instance_rate_limiting: true
  redis_host: "redis"
  redis_port: 6379
  redis_password: null
  
  # File Permissions & Monitoring
  enable_file_permissions: true
  prometheus_port: 4001
  track_end_users_prometheus: false

# LiteLLM Settings - Production Optimized
litellm_settings:
  request_timeout: 600
  set_verbose: false
  json_logs: true
  cache: true
  cache_params:
    type: redis
    host: "redis"
    port: 6379
    password: null

# ===================================================================
# SMART ALIAS STRATEGY - 25 Direct Task-to-Model Mappings
# Format: [task]_[profile] -> Direct Provider Model Assignment
# Admin can change any assignment directly in LiteLLM UI
# ===================================================================

model_list:
  # ===============================
  # CLASSIFICATION TASK MODELS
  # High-speed, cost-effective models for document classification
  # ===============================
  
  - model_name: "classification_premium"
    litellm_params:
      model: "openai/gpt-4o"
      api_key: "os.environ/OPENAI_API_KEY"
      temperature: 0.1
      max_tokens: 2048
    model_info:
      mode: "chat"
      supports_function_calling: true
      cost_per_token: 0.000005
      rpm: 500
      tpm: 800000
  
  - model_name: "classification_balanced"
    litellm_params:
      model: "gemini/gemini-1.5-pro-latest"
      api_key: "os.environ/GEMINI_API_KEY"
      temperature: 0.1
      max_tokens: 2048
    model_info:
      mode: "chat"
      supports_function_calling: true
      cost_per_token: 0.0000035
      rpm: 300
      tpm: 300000
  
  - model_name: "classification_cost_effective"
    litellm_params:
      model: "gemini/gemini-1.5-flash-latest"
      api_key: "os.environ/GEMINI_API_KEY"
      temperature: 0.1
      max_tokens: 2048
    model_info:
      mode: "chat"
      supports_function_calling: true
      cost_per_token: 0.0000002
      rpm: 1000
      tpm: 1000000
  
  - model_name: "classification_specialized"
    litellm_params:
      model: "anthropic/claude-3-haiku-20240307"
      api_key: "os.environ/ANTHROPIC_API_KEY"
      temperature: 0.1
      max_tokens: 2048
    model_info:
      mode: "chat"
      supports_function_calling: true
      cost_per_token: 0.00000025
      rpm: 400
      tpm: 400000
  
  - model_name: "classification_ultra_fast"
    litellm_params:
      model: "openai/gpt-4o-mini"
      api_key: "os.environ/OPENAI_API_KEY"
      temperature: 0.1
      max_tokens: 2048
    model_info:
      mode: "chat"
      supports_function_calling: true
      cost_per_token: 0.00000015
      rpm: 500
      tpm: 200000

  # ===============================
  # EXTRACTION TASK MODELS
  # Balanced performance for structured data extraction
  # ===============================
  
  - model_name: "extraction_premium"
    litellm_params:
      model: "openai/gpt-4o"
      api_key: "os.environ/OPENAI_API_KEY"
      temperature: 0.2
      max_tokens: 4096
    model_info:
      mode: "chat"
      supports_function_calling: true
      supports_json_schema: true
      cost_per_token: 0.000005
      rpm: 500
      tpm: 800000
  
  - model_name: "extraction_balanced"
    litellm_params:
      model: "gemini/gemini-1.5-pro-latest"
      api_key: "os.environ/GEMINI_API_KEY"
      temperature: 0.2
      max_tokens: 4096
    model_info:
      mode: "chat"
      supports_function_calling: true
      supports_json_schema: true
      cost_per_token: 0.0000035
      rpm: 300
      tpm: 300000
  
  - model_name: "extraction_cost_effective"
    litellm_params:
      model: "gemini/gemini-1.5-flash-latest"
      api_key: "os.environ/GEMINI_API_KEY"
      temperature: 0.2
      max_tokens: 4096
    model_info:
      mode: "chat"
      supports_function_calling: true
      cost_per_token: 0.0000002
      rpm: 1000
      tpm: 1000000
  
  - model_name: "extraction_specialized"
    litellm_params:
      model: "anthropic/claude-3-5-sonnet-20241022"
      api_key: "os.environ/ANTHROPIC_API_KEY"
      temperature: 0.2
      max_tokens: 4096
    model_info:
      mode: "chat"
      supports_function_calling: true
      cost_per_token: 0.000003
      rpm: 400
      tpm: 400000
  
  - model_name: "extraction_ultra_fast"
    litellm_params:
      model: "openai/gpt-4o-mini"
      api_key: "os.environ/OPENAI_API_KEY"
      temperature: 0.2
      max_tokens: 4096
    model_info:
      mode: "chat"
      supports_function_calling: true
      cost_per_token: 0.00000015
      rpm: 500
      tpm: 200000

  # ===============================
  # SYNTHESIS TASK MODELS
  # Premium models for high-quality text generation
  # ===============================
  
  - model_name: "synthesis_premium"
    litellm_params:
      model: "openai/gpt-4o"
      api_key: "os.environ/OPENAI_API_KEY"
      temperature: 0.7
      max_tokens: 8192
    model_info:
      mode: "chat"
      supports_streaming: true
      supports_function_calling: true
      cost_per_token: 0.000005
      rpm: 500
      tpm: 800000
  
  - model_name: "synthesis_balanced"
    litellm_params:
      model: "anthropic/claude-3-5-sonnet-20241022"
      api_key: "os.environ/ANTHROPIC_API_KEY"
      temperature: 0.7
      max_tokens: 8192
    model_info:
      mode: "chat"
      supports_streaming: true
      supports_function_calling: true
      cost_per_token: 0.000003
      rpm: 400
      tpm: 400000
  
  - model_name: "synthesis_cost_effective"
    litellm_params:
      model: "gemini/gemini-1.5-flash-latest"
      api_key: "os.environ/GEMINI_API_KEY"
      temperature: 0.7
      max_tokens: 8192
    model_info:
      mode: "chat"
      supports_streaming: true
      cost_per_token: 0.0000002
      rpm: 1000
      tpm: 1000000
  
  - model_name: "synthesis_specialized"
    litellm_params:
      model: "anthropic/claude-3-haiku-20240307"
      api_key: "os.environ/ANTHROPIC_API_KEY"
      temperature: 0.7
      max_tokens: 8192
    model_info:
      mode: "chat"
      supports_streaming: true
      cost_per_token: 0.00000025
      rpm: 400
      tpm: 400000
  
  - model_name: "synthesis_ultra_fast"
    litellm_params:
      model: "openai/gpt-4o-mini"
      api_key: "os.environ/OPENAI_API_KEY"
      temperature: 0.7
      max_tokens: 8192
    model_info:
      mode: "chat"
      supports_streaming: true
      cost_per_token: 0.00000015
      rpm: 500
      tpm: 200000

  # ===============================
  # VALIDATION_PRIMARY TASK MODELS
  # High-precision models for validation tasks
  # ===============================
  
  - model_name: "validation_primary_premium"
    litellm_params:
      model: "anthropic/claude-3-5-sonnet-20241022"
      api_key: "os.environ/ANTHROPIC_API_KEY"
      temperature: 0.1
      max_tokens: 4096
    model_info:
      mode: "chat"
      supports_function_calling: true
      cost_per_token: 0.000003
      rpm: 400
      tpm: 400000
  
  - model_name: "validation_primary_balanced"
    litellm_params:
      model: "openai/gpt-4o"
      api_key: "os.environ/OPENAI_API_KEY"
      temperature: 0.1
      max_tokens: 4096
    model_info:
      mode: "chat"
      supports_function_calling: true
      cost_per_token: 0.000005
      rpm: 500
      tpm: 800000
  
  - model_name: "validation_primary_cost_effective"
    litellm_params:
      model: "gemini/gemini-1.5-pro-latest"
      api_key: "os.environ/GEMINI_API_KEY"
      temperature: 0.1
      max_tokens: 4096
    model_info:
      mode: "chat"
      supports_function_calling: true
      cost_per_token: 0.0000035
      rpm: 300
      tpm: 300000
  
  - model_name: "validation_primary_specialized"
    litellm_params:
      model: "anthropic/claude-3-haiku-20240307"
      api_key: "os.environ/ANTHROPIC_API_KEY"
      temperature: 0.1
      max_tokens: 4096
    model_info:
      mode: "chat"
      supports_function_calling: true
      cost_per_token: 0.00000025
      rpm: 400
      tpm: 400000
  
  - model_name: "validation_primary_ultra_fast"
    litellm_params:
      model: "gemini/gemini-1.5-flash-latest"
      api_key: "os.environ/GEMINI_API_KEY"
      temperature: 0.1
      max_tokens: 4096
    model_info:
      mode: "chat"
      supports_function_calling: true
      cost_per_token: 0.0000002
      rpm: 1000
      tpm: 1000000

  # ===============================
  # VALIDATION_SECONDARY TASK MODELS
  # Secondary validation with different model perspectives
  # ===============================
  
  - model_name: "validation_secondary_premium"
    litellm_params:
      model: "openai/gpt-4o"
      api_key: "os.environ/OPENAI_API_KEY"
      temperature: 0.1
      max_tokens: 4096
    model_info:
      mode: "chat"
      supports_function_calling: true
      cost_per_token: 0.000005
      rpm: 500
      tpm: 800000
  
  - model_name: "validation_secondary_balanced"
    litellm_params:
      model: "gemini/gemini-1.5-pro-latest"
      api_key: "os.environ/GEMINI_API_KEY"
      temperature: 0.1
      max_tokens: 4096
    model_info:
      mode: "chat"
      supports_function_calling: true
      cost_per_token: 0.0000035
      rpm: 300
      tpm: 300000
  
  - model_name: "validation_secondary_cost_effective"
    litellm_params:
      model: "anthropic/claude-3-haiku-20240307"
      api_key: "os.environ/ANTHROPIC_API_KEY"
      temperature: 0.1
      max_tokens: 4096
    model_info:
      mode: "chat"
      supports_function_calling: true
      cost_per_token: 0.00000025
      rpm: 400
      tpm: 400000
  
  - model_name: "validation_secondary_specialized"
    litellm_params:
      model: "anthropic/claude-3-5-sonnet-20241022"
      api_key: "os.environ/ANTHROPIC_API_KEY"
      temperature: 0.1
      max_tokens: 4096
    model_info:
      mode: "chat"
      supports_function_calling: true
      cost_per_token: 0.000003
      rpm: 400
      tpm: 400000
  
  - model_name: "validation_secondary_ultra_fast"
    litellm_params:
      model: "openai/gpt-4o-mini"
      api_key: "os.environ/OPENAI_API_KEY"
      temperature: 0.1
      max_tokens: 4096
    model_info:
      mode: "chat"
      supports_function_calling: true
      cost_per_token: 0.00000015
      rpm: 500
      tpm: 200000

  # ===============================
  # EMBEDDINGS (Supporting Services)
  # High-quality embedding models for vector operations
  # ===============================
  
  - model_name: "embeddings_primary"
    litellm_params:
      model: "openai/text-embedding-3-large"
      api_key: "os.environ/OPENAI_API_KEY"
    model_info:
      mode: "embedding"
      cost_per_token: 0.00000013
      rpm: 3000
      tpm: 1000000
  
  - model_name: "embeddings_fast"
    litellm_params:
      model: "openai/text-embedding-3-small"
      api_key: "os.environ/OPENAI_API_KEY"
    model_info:
      mode: "embedding"
      cost_per_token: 0.00000002
      rpm: 3000
      tpm: 1000000

# ===================================================================
# ALIAS DOCUMENTATION FOR ADMIN REFERENCE
# ===================================================================
# 
# Total Models: 27 (25 Task Models + 2 Embedding Models)
# 
# TASK MODEL STRUCTURE:
# â€¢ 5 Tasks Ã— 5 Profiles = 25 Task Models
# â€¢ Tasks: classification, extraction, synthesis, validation_primary, validation_secondary
# â€¢ Profiles: premium, balanced, cost_effective, specialized, ultra_fast
# 
# ADMIN WORKFLOW:
# 1. Navigate to LiteLLM UI on http://localhost:3001
# 2. Go to "Models" section
# 3. Click "Edit" on any model (e.g., "synthesis_premium")
# 4. Change "model" field from "openai/gpt-4o" to desired provider model
# 5. Update API key if needed
# 6. Adjust temperature, max_tokens for specific use case
# 7. Save changes
# 
# RESULT: Immediate effect on next API call - no code changes needed!
# 
# =================================================================== 