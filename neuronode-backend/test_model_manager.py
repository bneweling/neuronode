#!/usr/bin/env python3
"""
Live Test f√ºr ModelManager
Testet die dynamische Modell-Aufl√∂sung mit der LiteLLM UI
"""

import asyncio
import sys
import os
import json
from datetime import datetime

# Add src to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from src.llm.model_manager import (
    ModelManager, 
    TaskType, 
    ModelTier, 
    get_model_manager
)

async def test_model_resolution():
    """Test die Modell-Aufl√∂sung f√ºr alle Task-Types"""
    
    print("üß™ TESTING: ModelManager Live Integration")
    print("=" * 60)
    
    try:
        # Get model manager
        model_manager = await get_model_manager()
        
        # Test alle Task-Types
        test_cases = [
            (TaskType.CLASSIFICATION, ModelTier.PREMIUM),
            (TaskType.EXTRACTION, ModelTier.BALANCED),
            (TaskType.SYNTHESIS, ModelTier.COST_EFFECTIVE),
            (TaskType.VALIDATION_PRIMARY, ModelTier.PREMIUM),
            (TaskType.VALIDATION_SECONDARY, ModelTier.BALANCED),
        ]
        
        results = []
        
        for task_type, model_tier in test_cases:
            print(f"\nüîç Testing: {task_type.value} (Tier: {model_tier.value})")
            
            try:
                model_config = await model_manager.get_model_for_task(
                    task_type=task_type,
                    model_tier=model_tier,
                    fallback=True
                )
                
                print(f"‚úÖ Resolved Model: {model_config['model']}")
                print(f"   Alias: {model_config['model_alias']}")
                print(f"   Tier: {model_config['tier']}")
                print(f"   Strategy: {model_config['selection_strategy']}")
                print(f"   Selected At: {model_config['selected_at']}")
                
                results.append({
                    "task_type": task_type.value,
                    "requested_tier": model_tier.value,
                    "success": True,
                    **model_config
                })
                
            except Exception as e:
                print(f"‚ùå Error: {e}")
                results.append({
                    "task_type": task_type.value,
                    "requested_tier": model_tier.value,
                    "success": False,
                    "error": str(e)
                })
        
        # Test Performance Stats
        print(f"\nüìä Performance Statistics:")
        stats = await model_manager.get_model_performance_stats()
        print(json.dumps(stats, indent=2))
        
        # Test Cache Refresh
        print(f"\nüîÑ Testing Cache Refresh...")
        await model_manager.refresh_config_cache()
        print("‚úÖ Cache refreshed successfully")
        
        # Summary
        print(f"\nüìã SUMMARY:")
        print("=" * 40)
        successful = sum(1 for r in results if r["success"])
        total = len(results)
        print(f"‚úÖ Successful resolutions: {successful}/{total}")
        
        if successful == total:
            print("üéâ ALL TESTS PASSED!")
            print("üöÄ ModelManager is FULLY OPERATIONAL!")
        else:
            print("‚ö†Ô∏è  Some tests failed - check configuration")
        
        # Clean up
        await model_manager.api_client.aclose()
        
        return results
        
    except Exception as e:
        print(f"üí• CRITICAL ERROR: {e}")
        import traceback
        traceback.print_exc()
        return []

async def main():
    """Main test function"""
    
    print(f"üïê Test started at: {datetime.now().isoformat()}")
    
    results = await test_model_resolution()
    
    print(f"\nüïê Test completed at: {datetime.now().isoformat()}")
    
    # Return appropriate exit code
    if results and all(r["success"] for r in results):
        sys.exit(0)  # Success
    else:
        sys.exit(1)  # Failure

if __name__ == "__main__":
    asyncio.run(main()) 