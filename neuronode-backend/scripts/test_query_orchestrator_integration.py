#!/usr/bin/env python3
"""
Enhanced Query Orchestrator Integration Test
============================================

Comprehensive end-to-end testing of the QueryOrchestrator with 
LiteLLM v1.72.6 integration, validating:

1. Complete RAG pipeline orchestration
2. Service coordination (IntentAnalyzer ‚Üí Retriever ‚Üí ResponseSynthesizer)
3. Request prioritization and performance tracking
4. Error handling and fallback strategies
5. Conversation context management
6. Caching and performance optimization

Test Categories:
- Unit tests for orchestration logic
- Integration tests with mocked services
- Performance benchmarks
- Error handling validation
- Conversation flow testing
"""

import asyncio
import time
import json
import sys
from pathlib import Path
from typing import Dict, Any, List, Optional
from unittest.mock import Mock, AsyncMock, patch
import logging

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.orchestration.query_orchestrator import QueryOrchestrator
from src.models.llm_models import QueryAnalysis, SynthesizedResponse, QueryIntent, EntityData
from src.config.exceptions import QueryProcessingError, ErrorCode

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MockIntentAnalyzer:
    """Mock for IntentAnalyzer with realistic behavior"""
    
    async def analyze_query(self, query: str) -> QueryAnalysis:
        """Mock intent analysis with realistic delay and results"""
        # Simulate pattern-based fast analysis (0.02ms as proven)
        await asyncio.sleep(0.0001)  # 0.1ms to simulate processing
        
        # Create realistic analysis based on query content
        if "BSI C5" in query or "Control" in query:
            intent = QueryIntent.COMPLIANCE_REQUIREMENT
            entities = [
                EntityData(text="BSI C5", entity_type="STANDARD", confidence=0.95),
                EntityData(text="Control", entity_type="CONCEPT", confidence=0.9)
            ]
            keywords = ["BSI", "C5", "Control", "Compliance"]
        elif "implementier" in query.lower():
            intent = QueryIntent.TECHNICAL_IMPLEMENTATION
            entities = [
                EntityData(text="Implementation", entity_type="CONCEPT", confidence=0.9)
            ]
            keywords = ["Implementation", "Technical"]
        else:
            intent = QueryIntent.GENERAL_INFORMATION
            entities = [
                EntityData(text="Information", entity_type="CONCEPT", confidence=0.8)
            ]
            keywords = ["General", "Information"]
        
        return QueryAnalysis(
            primary_intent=intent,
            secondary_intents=[],
            entities=entities,
            search_keywords=keywords,
            requires_comparison=False,
            temporal_context=None,
            confidence=0.9,
            complexity_score=0.6
        )

class MockHybridRetriever:
    """Mock for HybridRetriever with realistic retrieval simulation"""
    
    async def retrieve(self, query: str, analysis: QueryAnalysis, max_results: int = 20) -> List[Dict[str, Any]]:
        """Mock retrieval with realistic delay and results"""
        # Simulate retrieval time (graph + vector search)
        await asyncio.sleep(0.05)  # 50ms realistic retrieval time
        
        # Generate mock results based on query analysis
        results = []
        for i in range(min(max_results, 5)):  # Return 5 mock results
            results.append({
                "content": f"Mock content {i+1} for query: {query[:50]}...",
                "source": f"mock_document_{i+1}.pdf",
                "type": "DOCUMENT",
                "confidence": 0.8 - (i * 0.1),
                "metadata": {
                    "page": str(i+1),
                    "section": f"Section {i+1}",
                    "intent_match": analysis.primary_intent.value
                }
            })
        
        return results

class MockResponseSynthesizer:
    """Mock for ResponseSynthesizer with realistic synthesis"""
    
    async def synthesize_response(
        self, 
        query: str, 
        analysis: QueryAnalysis, 
        retrieval_results: List[Dict[str, Any]],
        user_context: Optional[Dict[str, Any]] = None
    ) -> SynthesizedResponse:
        """Mock response synthesis with realistic delay and quality output"""
        # Simulate synthesis time (LLM processing with LOW priority)
        await asyncio.sleep(0.1)  # 100ms realistic synthesis time
        
        # Generate realistic response based on inputs
        answer = f"""Basierend auf Ihrer Anfrage "{query}" und der Analyse (Intent: {analysis.primary_intent.value}) 
        habe ich {len(retrieval_results)} relevante Informationen gefunden.

        Die wichtigsten Erkenntnisse sind:
        - {retrieval_results[0]['content'][:100] if retrieval_results else 'Keine spezifischen Inhalte gefunden'}...
        - Weitere Details finden Sie in den angegebenen Quellen.

        Diese Antwort wurde mit der Enhanced Response Synthesizer (LiteLLM v1.72.6) generiert."""
        
        sources = [
            {
                "source": result["source"],
                "type": result["type"],
                "confidence": result["confidence"],
                "page": result["metadata"].get("page", "1")
            }
            for result in retrieval_results[:3]  # Top 3 sources
        ]
        
        follow_up_questions = [
            "M√∂chten Sie weitere Details zu einem spezifischen Aspekt?",
            "Ben√∂tigen Sie Implementierungsbeispiele?",
            "Soll ich verwandte Controls oder Standards erl√§utern?"
        ]
        
        return SynthesizedResponse(
            answer=answer,
            sources=sources,
            confidence=0.85,
            follow_up_questions=follow_up_questions,
            metadata={
                "synthesis_model": "mock-claude-3-5-sonnet",
                "synthesis_time": 0.1,
                "retrieval_count": len(retrieval_results),
                "user_context": user_context
            }
        )

class QueryOrchestratorIntegrationTest:
    """Comprehensive integration test suite for QueryOrchestrator"""
    
    def __init__(self):
        self.test_results = {
            "total_tests": 0,
            "passed": 0,
            "failed": 0,
            "errors": [],
            "performance_metrics": {}
        }
    
    async def run_all_tests(self):
        """Run complete test suite"""
        logger.info("üöÄ Starting Enhanced Query Orchestrator Integration Tests")
        logger.info("=" * 80)
        
        # Test categories
        test_categories = [
            ("Basic Orchestration", self.test_basic_orchestration),
            ("Service Integration", self.test_service_integration),
            ("Performance Tracking", self.test_performance_tracking),
            ("Error Handling", self.test_error_handling),
            ("Conversation Context", self.test_conversation_context),
            ("Caching System", self.test_caching_system),
            ("Legacy Compatibility", self.test_legacy_compatibility)
        ]
        
        for category_name, test_method in test_categories:
            logger.info(f"\nüìã Testing Category: {category_name}")
            logger.info("-" * 50)
            
            try:
                await test_method()
                logger.info(f"‚úÖ {category_name} tests completed successfully")
            except Exception as e:
                logger.error(f"‚ùå {category_name} tests failed: {e}")
                self.test_results["errors"].append(f"{category_name}: {str(e)}")
        
        # Generate final report
        self.generate_test_report()
    
    async def test_basic_orchestration(self):
        """Test basic end-to-end orchestration"""
        orchestrator = self._create_mock_orchestrator()
        
        # Test 1: Simple query orchestration
        start_time = time.time()
        result = await orchestrator.orchestrate_query(
            user_query="Was fordert BSI C5 zu Zugriffskontrollen?",
            user_context={"test_case": "basic_orchestration"}
        )
        processing_time = time.time() - start_time
        
        # Validate result structure
        assert "query" in result
        assert "response" in result
        assert "sources" in result
        assert "confidence" in result
        assert "metadata" in result
        assert "analysis" in result
        
        # Validate metadata
        assert result["metadata"]["orchestrator_version"] == "QueryOrchestrator_v1.72.6"
        assert result["metadata"]["litellm_integration"] is True
        assert "performance_breakdown" in result["metadata"]
        
        # Validate performance breakdown
        perf = result["metadata"]["performance_breakdown"]
        assert "intent_analysis_ms" in perf
        assert "retrieval_ms" in perf
        assert "synthesis_ms" in perf
        assert "total_ms" in perf
        
        # Performance validation (should be fast with mocks)
        assert processing_time < 1.0  # Should complete in under 1 second with mocks
        assert perf["intent_analysis_ms"] < 50  # Intent analysis should be very fast
        
        self.test_results["total_tests"] += 1
        self.test_results["passed"] += 1
        
        logger.info(f"‚úÖ Basic orchestration completed in {processing_time*1000:.1f}ms")
        logger.info(f"   Intent Analysis: {perf['intent_analysis_ms']}ms")
        logger.info(f"   Retrieval: {perf['retrieval_ms']}ms")
        logger.info(f"   Synthesis: {perf['synthesis_ms']}ms")
    
    async def test_service_integration(self):
        """Test integration between all services"""
        orchestrator = self._create_mock_orchestrator()
        
        # Test different query types to validate service coordination
        test_queries = [
            ("BSI C5 Zugriffskontrollen", QueryIntent.COMPLIANCE_REQUIREMENT),
            ("Wie implementiere ich Multi-Factor Authentication?", QueryIntent.TECHNICAL_IMPLEMENTATION),
            ("Was sind die Unterschiede zwischen ISO 27001 und BSI C5?", QueryIntent.GENERAL_INFORMATION)
        ]
        
        for query, expected_intent in test_queries:
            result = await orchestrator.orchestrate_query(user_query=query)
            
            # Validate intent was correctly identified
            assert result["analysis"]["intent"] == expected_intent.value
            
            # Validate response quality
            assert len(result["response"]) > 50  # Substantial response
            assert result["confidence"] > 0.7  # High confidence
            assert len(result["sources"]) > 0  # Has sources
            
            self.test_results["total_tests"] += 1
            self.test_results["passed"] += 1
            
            logger.info(f"‚úÖ Service integration validated for intent: {expected_intent.value}")
    
    async def test_performance_tracking(self):
        """Test performance tracking and statistics"""
        orchestrator = self._create_mock_orchestrator()
        
        # Run multiple queries to build statistics
        for i in range(5):
            await orchestrator.orchestrate_query(f"Test query {i}")
        
        # Get performance stats
        stats = orchestrator.get_performance_stats()
        
        # Validate statistics structure
        assert "total_queries" in stats
        assert "cache_hits" in stats
        assert "avg_processing_time" in stats
        assert "error_rate" in stats
        assert "cache_hit_rate" in stats
        assert "timestamp" in stats
        
        # Validate statistics values
        assert stats["total_queries"] == 5
        assert stats["avg_processing_time"] > 0
        assert stats["error_rate"] == 0.0  # No errors in successful tests
        
        self.test_results["total_tests"] += 1
        self.test_results["passed"] += 1
        self.test_results["performance_metrics"] = stats
        
        logger.info(f"‚úÖ Performance tracking validated")
        logger.info(f"   Total Queries: {stats['total_queries']}")
        logger.info(f"   Avg Processing Time: {stats['avg_processing_time']*1000:.1f}ms")
        logger.info(f"   Cache Hit Rate: {stats['cache_hit_rate']*100:.1f}%")
    
    async def test_error_handling(self):
        """Test error handling and fallback strategies"""
        # Create orchestrator with failing mock services
        failing_analyzer = Mock()
        failing_analyzer.analyze_query = AsyncMock(side_effect=Exception("Mock analyzer failure"))
        
        orchestrator = QueryOrchestrator(
            intent_analyzer=failing_analyzer,
            hybrid_retriever=MockHybridRetriever(),
            response_synthesizer=MockResponseSynthesizer()
        )
        
        # Test error handling
        result = await orchestrator.orchestrate_query("Test query")
        
        # Validate error response structure
        assert result["metadata"]["error"] is True
        assert "error_code" in result["metadata"]
        assert "error_type" in result["metadata"]
        assert result["confidence"] == 0.0
        assert len(result["follow_up_questions"]) > 0
        
        # Validate user-friendly error message
        assert "unerwarteter Systemfehler" in result["response"]
        
        self.test_results["total_tests"] += 1
        self.test_results["passed"] += 1
        
        logger.info("‚úÖ Error handling validated")
        logger.info(f"   Error Code: {result['metadata']['error_code']}")
        logger.info(f"   Error Type: {result['metadata']['error_type']}")
    
    async def test_conversation_context(self):
        """Test conversation context management"""
        orchestrator = self._create_mock_orchestrator()
        
        # Simulate conversation flow
        messages = [
            {"role": "user", "content": "Was ist BSI C5?"},
            {"role": "assistant", "content": "BSI C5 ist ein deutscher Cloud-Sicherheitsstandard..."},
            {"role": "user", "content": "Welche Controls gibt es daf√ºr?"}
        ]
        
        result = await orchestrator.process_conversation(
            messages=messages,
            conversation_id="test-conversation-123"
        )
        
        # Validate conversation context integration
        assert result["metadata"]["user_context"]["conversation_id"] == "test-conversation-123"
        assert result["metadata"]["user_context"]["message_count"] == 3
        assert result["metadata"]["user_context"]["has_history"] is True
        
        # The enhanced query should contain context
        # This is validated internally by the orchestrator
        
        self.test_results["total_tests"] += 1
        self.test_results["passed"] += 1
        
        logger.info("‚úÖ Conversation context management validated")
        logger.info(f"   Conversation ID: {result['metadata']['user_context']['conversation_id']}")
        logger.info(f"   Message Count: {result['metadata']['user_context']['message_count']}")
    
    async def test_caching_system(self):
        """Test intelligent caching system"""
        orchestrator = self._create_mock_orchestrator()
        
        query = "Test query for caching"
        
        # First call - should not be cached
        result1 = await orchestrator.orchestrate_query(query)
        assert result1["metadata"].get("cached", False) is False
        
        # Second call - should be cached (if confidence > 0.7)
        result2 = await orchestrator.orchestrate_query(query)
        
        if result1["confidence"] > 0.7:
            assert result2["metadata"].get("cached", False) is True
            logger.info("‚úÖ Caching system validated - cache hit detected")
        else:
            logger.info("‚úÖ Caching system validated - low confidence response not cached")
        
        # Validate cache statistics
        stats = orchestrator.get_performance_stats()
        if result1["confidence"] > 0.7:
            assert stats["cache_hits"] > 0
            assert stats["cache_hit_rate"] > 0
        
        self.test_results["total_tests"] += 1
        self.test_results["passed"] += 1
    
    async def test_legacy_compatibility(self):
        """Test legacy compatibility wrapper"""
        orchestrator = self._create_mock_orchestrator()
        
        # Test legacy process_query method
        result = await orchestrator.process_query(
            query="Legacy compatibility test",
            user_context={"legacy": True}
        )
        
        # Should return same structure as orchestrate_query
        assert "query" in result
        assert "response" in result
        assert "metadata" in result
        assert result["metadata"]["orchestrator_version"] == "QueryOrchestrator_v1.72.6"
        
        self.test_results["total_tests"] += 1
        self.test_results["passed"] += 1
        
        logger.info("‚úÖ Legacy compatibility validated")
    
    def _create_mock_orchestrator(self) -> QueryOrchestrator:
        """Create orchestrator with mock services for testing"""
        return QueryOrchestrator(
            intent_analyzer=MockIntentAnalyzer(),
            hybrid_retriever=MockHybridRetriever(),
            response_synthesizer=MockResponseSynthesizer()
        )
    
    def generate_test_report(self):
        """Generate comprehensive test report"""
        logger.info("\n" + "=" * 80)
        logger.info("üèÅ ENHANCED QUERY ORCHESTRATOR INTEGRATION TEST REPORT")
        logger.info("=" * 80)
        
        # Test summary
        logger.info(f"üìä Test Summary:")
        logger.info(f"   Total Tests: {self.test_results['total_tests']}")
        logger.info(f"   Passed: {self.test_results['passed']}")
        logger.info(f"   Failed: {self.test_results['failed']}")
        logger.info(f"   Success Rate: {(self.test_results['passed']/max(self.test_results['total_tests'], 1))*100:.1f}%")
        
        # Performance metrics
        if self.test_results["performance_metrics"]:
            perf = self.test_results["performance_metrics"]
            logger.info(f"\n‚ö° Performance Metrics:")
            logger.info(f"   Average Processing Time: {perf['avg_processing_time']*1000:.1f}ms")
            logger.info(f"   Cache Hit Rate: {perf['cache_hit_rate']*100:.1f}%")
            logger.info(f"   Error Rate: {perf['error_rate']*100:.1f}%")
        
        # Errors
        if self.test_results["errors"]:
            logger.info(f"\n‚ùå Errors:")
            for error in self.test_results["errors"]:
                logger.info(f"   - {error}")
        
        # Final assessment
        if self.test_results["failed"] == 0:
            logger.info(f"\nüéâ ALL TESTS PASSED! Enhanced Query Orchestrator is ready for production.")
            logger.info(f"‚úÖ LiteLLM v1.72.6 integration validated successfully")
            logger.info(f"‚úÖ End-to-end orchestration pipeline operational")
            logger.info(f"‚úÖ Performance tracking and error handling verified")
        else:
            logger.info(f"\n‚ö†Ô∏è  Some tests failed. Please review errors above.")
        
        logger.info("=" * 80)

async def main():
    """Run the integration test suite"""
    test_suite = QueryOrchestratorIntegrationTest()
    await test_suite.run_all_tests()

if __name__ == "__main__":
    asyncio.run(main()) 